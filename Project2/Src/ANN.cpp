#include "common.h"
#include "ANN.h"
#include "WordClassifier.h"
#include "File.h"

ANN::Neuron::Neuron(unsigned int prevLayerSize) :
	activation(0)
{
    //Initializing everything randomly if this isnâ€™t an input neuron
    if(prevLayerSize) {
        for(int i = 0; i < prevLayerSize; i++)
            weights.push_back(randToOne());
        bias = randToOne();
    }
}

ANN::Neuron::Neuron(const Neuron &a) :
	activation(a.activation), bias(a.bias)
{
	weights = a.weights;
}

//Public:
ANN::ANN(unsigned int nbHLayers, unsigned int nbInputNeurons, unsigned int nbHiddenNeurons, unsigned int nbOutputNeurons, float learningRate) :
	m_nbHLayers(nbHLayers), m_learningRate(learningRate) 
{
	initLayerSizes(nbInputNeurons, nbHiddenNeurons, nbOutputNeurons);
    initNeurons();
}

ANN::ANN(const TrainingSet *trainingSet, unsigned int nbHLayers, unsigned  nbHiddenNeurons, float learningRate) :
	m_nbHLayers(nbHLayers), m_learningRate(learningRate)
{
    if(!nbHiddenNeurons)
        nbHiddenNeurons = (int)(1.5 * (float)trainingSet->maxInputSize) + 1;
	initLayerSizes(trainingSet->maxInputSize, nbHiddenNeurons, 1);
	initNeurons();
    train(trainingSet, 1);
}

ANN::ANN(const vector<unsigned int> &layerSizes, float learningRate) :
	m_layerSizes(layerSizes), m_learningRate(learningRate)
{
	m_nbHLayers = layerSizes.size() - 2;
	initNeurons();
}

ANN::ANN(const ANN &a) :
	m_nbHLayers(a.m_nbHLayers), m_learningRate(a.m_learningRate)
{
	//Copying neurons
	m_neurons = vector<vector<Neuron*>>(a.m_nbHLayers + 2);
	for (int i = 0; i < a.m_nbHLayers + 2; i++) {
		for (int j = 0; j < a.m_layerSizes[i]; j++) {
			m_neurons[i].push_back(new Neuron(*(a.m_neurons[i][j])));
		}
	}
	m_layerSizes = a.m_layerSizes;
	m_outputMap = a.m_outputMap;
}

ANN::~ANN() {
    for(int i = 0; i < m_nbHLayers + 2; i++) {
        for(int j = 0; j < m_layerSizes[i]; j++)
            delete m_neurons[i][j];
    }
}

unsigned int ANN::getNbInputNeurons() const {
	if (!m_layerSizes.empty())
		return m_layerSizes[0];
	return 0;
}

void ANN::initLayerSizes(unsigned int nbInputNeurons, unsigned int nbHiddenNeurons, unsigned int nbOutputNeurons) {
	//Initializing a vector with all the layers' size
	m_layerSizes.push_back(nbInputNeurons);
	for (int i = 0; i < m_nbHLayers; i++)
		m_layerSizes.push_back(nbHiddenNeurons);
	m_layerSizes.push_back(nbOutputNeurons);
}

void ANN::initNeurons() {
    //Creating the neurons
    vector<Neuron*> temp;
    for(int i = 0; i < m_layerSizes[0]; i++)
        temp.push_back(new Neuron(0));
	m_neurons.push_back(temp);
    temp.clear();
    for(int i = 1; i < m_nbHLayers + 2; i++) {
        for(int j = 0; j < m_layerSizes[i]; j++)
            temp.push_back(new Neuron(m_layerSizes[i - 1]));
		m_neurons.push_back(temp);
        temp.clear();
    }
}

void ANN::train(const TrainingSet *trainingSet, unsigned int nbIterations) {
	/* By applying gradient descent we can find the optimal configuration of weights and biases 
	for each training sample. We can then find the overall weights and biases by averaging those of 
	all these optimal configs. */
	m_outputMap = trainingSet->outputMap;
	unsigned int progress = 0;
	vector<ANN> optimizations; //Stores the optimized config of the neural network for each sample
	cout << "Training the neural network:" << endl;
	cout << "Computing optimizations... (1/2)" << endl;
	thread progressBarThread(&progressBar, &progress, trainingSet->nbSamples);
	for (int j = 0; j < trainingSet->nbSamples; j++) {
		ANN optimization = *this;
		//Update weights until the cost doesn't change anymore
		float cost = 1;
		while (1) {
			optimization.propagate(trainingSet->samples[j].input);
			optimization.backpropagate(trainingSet->samples[j].output, m_nbHLayers + 1);
			float new_cost = optimization.findCost(trainingSet->samples[j].output);
			if (new_cost >= cost)
				break;
			cost = new_cost;
		}
		optimizations.push_back(optimization);
		progress++;
	}
	progressBarThread.join();
	cout << "Updating the neural network... (2/2)" << endl;
	//Calculate the average of the weights and biases between all the optimizations
	updateNetwork(optimizations);
	cout << "Done!" << endl;
}

string ANN::probeClassification(const vector<float> &inputs) {
	//Propagating inputs through the neural network
	//Returning the string of the possible output which is the closest to the real output
	if (!m_outputMap.size())
		return string();
	propagate(inputs);
	return mapOutput(m_neurons[m_nbHLayers + 1][0]->activation);
}

vector<float> ANN::probeOutputs(const vector<float> &inputs) {
	//Propagating inputs through the neural network
	//Returning a vector with the activations of the output layer
	vector<float> outputs;
	propagate(inputs);
	for (int i = 0; i < m_layerSizes[m_nbHLayers + 1]; i++)
		outputs.push_back(m_neurons[m_nbHLayers + 1][i]->activation);
	return outputs;
}

//Private:
void ANN::propagate(const vector<float> &inputs) {
	//Setting the first layer's neurons' activations to the value of the inputs
	for (int i = 0; i < m_layerSizes[0]; i++) {
		if (i < inputs.size())
			m_neurons[0][i]->activation = inputs[i];
		else {
			m_neurons[0][i]->activation = 1;
		}
	}
	//Propagating the input layer through the other layers
	for (int i = 1; i < m_nbHLayers + 2; i++) {
		for (int j = 0; j < m_layerSizes[i]; j++) {
			Neuron *curr = m_neurons[i][j];
			float activation = curr->bias;
			for (int k = 0; k < m_layerSizes[i - 1]; k++)
				activation += m_neurons[i - 1][k]->activation * curr->weights[k];
			curr->activation = activate(activation, i);
		}
	}
}

void ANN::propagate(unsigned int startingLayer) {
	//Propagateing, but only starting from startingLayer
	for (int i = startingLayer; i < m_nbHLayers + 2; i++) {
		for (int j = 0; j < m_layerSizes[i]; j++) {
			Neuron *curr = m_neurons[i][j];
			float activation = curr->bias;
			for (int k = 0; k < m_layerSizes[i - 1]; k++)
				activation += m_neurons[i - 1][k]->activation * curr->weights[k];
			curr->activation = activate(activation, i);
		}
	}
}

void ANN::backpropagate(const vector<float> &outputs, unsigned int layer) {
	//Initializing a multidimensional vector of gradients
	static vector<vector<vector<float>>> gradients;
	gradients = vector<vector<vector<float>>>(m_nbHLayers + 2);
	for (int i = 0; i < m_nbHLayers + 2; i++)
		gradients[i] = vector<vector<float>>(m_layerSizes[i]);
	float dBase, derivative;
	//Calculating the gradients of the output layer
	int nbOutputNeurons = m_layerSizes[m_nbHLayers + 1];
	for (int i = 0; i < nbOutputNeurons; i++) {
		//dBase (dC/dz) = dz(cost(act)) = da(cost) * dz(act)
		dBase = (m_neurons[m_nbHLayers + 1][i]->activation - outputs[i]);
		dBase *= (1 / (float)(m_layerSizes[m_nbHLayers] + 1));
		for (int j = 0; j < m_layerSizes[m_nbHLayers]; j++) {
			//derivative (dC/dw) = dBase * dw(wSums)
			derivative = dBase * m_neurons[m_nbHLayers][j]->activation;
			gradients[m_nbHLayers + 1][i].push_back(derivative);
		}
		//derivative (dC/db) = dBase * db(wSums)
		gradients[m_nbHLayers + 1][i].push_back(dBase);
	}
	//Updating weights of output neurons
	if (layer == m_nbHLayers + 1) {
		for (int i = 0; i < nbOutputNeurons; i++)
			updateWeights(gradients[m_nbHLayers + 1][i], m_neurons[m_nbHLayers + 1][i]);
		//Repropagating
		propagate(m_nbHLayers + 1);
		//Backpropagate layers before layer
		backpropagate(outputs, layer - 1);
		return;
	}
	//Calculating the gradients of the hidden layers and updating the weights
	for (int i = m_nbHLayers; i > 0; i--) {
		for (int j = 0; j < m_layerSizes[i]; j++) {
			//dBase (dC/da) where a is the activation of this neuron
			dBase = 0;
			for (int k = 0; k < m_layerSizes[i + 1]; k++)
				dBase += gradients[i + 1][k][j] * m_neurons[i + 1][k]->weights[j];
			//dBase (dC/dz) = dBase * dz(wSums)
			dBase *= (1 / (float)(m_layerSizes[i - 1] + 1));
			for (int k = 0; k < m_layerSizes[i - 1]; k++) {
				//derivative (dC/dw) = dBase * dw(wSums)
				derivative = dBase * m_neurons[i - 1][k]->activation;
				gradients[i][j].push_back(derivative);
			}
			gradients[i][j].push_back(dBase);
		}
		//Updating weights
		if (layer == i) {
			for (int j = 0; j < m_layerSizes[i]; j++)
				updateWeights(gradients[i][j], m_neurons[i][j]);
			//Repropagate
			propagate(i);
			//Backpropagate layers before layer
			backpropagate(outputs, layer - 1);
			return;
		}
	}
}

string ANN::mapOutput(float output) {
	//Finding the string of the possible output which is the closest to the output of the neural network
    if(m_outputMap.size() == 0)
        return "";
    float smallestDistance = 1;
    string ret;
    for(map<float, string>::iterator it = m_outputMap.begin(); it != m_outputMap.end(); it++) {
        float curr = it->first;
        if(output > curr)
            curr = output - curr;
        else
            curr -= output;
        if(curr < smallestDistance) {
            smallestDistance = curr;
            ret = it->second;
        }
    }
    return ret;
}

float ANN::activate(float input, int layer) {
	//Dividing the weighted sum by the number of neurons in the previous layer and adding 1 to take into account the bias
	///The maximum value that a weighted sum can give is when all the weights, all the activations and the bias are equal to 1
	//Ex: s = w1*a1 + w2*a2 + b = 1*1 + 1*1 + 1 = 3
    return input / (m_layerSizes[layer - 1] + 1);
}

void ANN::updateWeights(const vector<float> &gradient, Neuron *neuron) {
	//Making small changes to the weights and the bias that are proportional to dC/dw or dC/db
	float updatedWeight, updatedBias;
	for (int i = 0; i < neuron->weights.size(); i++) {
		updatedWeight = neuron->weights[i] + (-gradient[i] * m_learningRate * 100);
		//Making sure that the updated weight sits between 0 and 1
		if (updatedWeight < 0)
			neuron->weights[i] = 0;
		else if(updatedWeight > 1)
			neuron->weights[i] = 1;
		else
			neuron->weights[i] = updatedWeight;
	}
	updatedBias = neuron->bias + (-gradient.back() * m_learningRate * 10);
	//Making sure that the updated bias sits between 0 and 1
	if (updatedBias < 0)
		neuron->bias = 0;
	else if (updatedBias > 1)
		neuron->bias = 1;
	else
		neuron->bias = updatedBias;
}

float ANN::findCost(const vector<float> &outputs) {
	//C = 0.5(a - t)^2 where a is the activation of an output neuron and t, what it's supposed to be
	//When there's multiple output neurons, the cost will be the sum of every output neuron's cost
	float ret = 0;
	vector<Neuron*> &outputLayer = m_neurons[m_nbHLayers + 1];
	for (int i = 0; i < outputLayer.size(); i++)
		ret += (0.5 * (outputLayer[i]->activation - outputs[i]) * (outputLayer[i]->activation - outputs[i]));
	return ret;
}

void ANN::updateNetwork(const vector<ANN> &optimizations) {
	//Cycling through all the neurons to update their weights and bias with their average between all the optimizations/configs
	unsigned int nbSamples = optimizations.size();
	for (int i = 1; i < m_nbHLayers; i++) { //Cycling through the layers
		for (int j = 0; j < m_layerSizes[i]; j++) { //Cycling through the neurons of a layer
			for (int k = 0; k < m_layerSizes[i - 1]; k++) { //Cycling through the weights of a neuron
				//Calculating the average for the weight
				float average = 0;
				for (int l = 0; l < nbSamples; l++)
					average += optimizations[l].m_neurons[i][j]->weights[k];
				m_neurons[i][j]->weights[k] = average / nbSamples;
			}
			//Calculating the average for the bias
			float average = 0;
			for (int l = 0; l < nbSamples; l++)
				average += optimizations[l].m_neurons[i][j]->bias;
			m_neurons[i][j]->bias = average / nbSamples;
		}
	}
}